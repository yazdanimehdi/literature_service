# Literature Review Service - Environment Variables
#
# Copy this file to .env and fill in your values.
# All non-secret settings are configured in config/config.yaml.
#
# Variable naming: LITREVIEW_<SECTION>_<KEY>
# Any config.yaml value can also be overridden via env var using this pattern.
# Example: llm.timeout -> LITREVIEW_LLM_TIMEOUT=120s

# =============================================================================
# Database
# =============================================================================

LITREVIEW_DATABASE_PASSWORD=

# =============================================================================
# LLM Provider API Keys
# =============================================================================
# Only set the key for the provider you are using (configured in config.yaml).

# OpenAI (provider: openai)
LITREVIEW_LLM_OPENAI_API_KEY=

# Anthropic (provider: anthropic)
LITREVIEW_LLM_ANTHROPIC_API_KEY=

# Azure OpenAI (provider: azure)
LITREVIEW_LLM_AZURE_API_KEY=

# Google Gemini (provider: gemini)
# For Vertex AI mode, use GCP Application Default Credentials instead.
LITREVIEW_LLM_GEMINI_API_KEY=

# AWS Bedrock (provider: bedrock)
# Uses standard AWS credential chain (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY,
# AWS_SESSION_TOKEN, or instance profile). No LITREVIEW_ key needed.

# =============================================================================
# Embedding Configuration
# =============================================================================
# The embedding provider defaults to "openai". Set to use a different provider
# for vector embeddings (used by semantic dedup and paper embedding activities).
#
# Supported providers: openai, azure, bedrock, gemini, vertex
# LITREVIEW_LLM_EMBEDDING_PROVIDER=openai
# LITREVIEW_LLM_EMBEDDING_MODEL=text-embedding-3-small
#
# Provider-specific notes:
# - openai: Uses the OpenAI API key above (LITREVIEW_LLM_OPENAI_API_KEY).
# - azure: Uses the Azure API key above (LITREVIEW_LLM_AZURE_API_KEY).
#          Uses the same resource_name (endpoint) as the chat provider.
#          Set a separate embedding deployment name in config.yaml:
#            azure.embedding_deployment_name: "text-embedding-3-small"
#          Or via env var:
#            LITREVIEW_LLM_AZURE_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small
# - bedrock: Uses the AWS credential chain (same as LLM chat provider).
#            Configure region in config.yaml.
# - gemini: Uses the Gemini API key above (LITREVIEW_LLM_GEMINI_API_KEY).
# - vertex: Uses GCP Application Default Credentials.
#           Configure project/location in config.yaml.

# =============================================================================
# Paper Source API Keys
# =============================================================================
# All optional. Some sources work without keys but have lower rate limits.

# Semantic Scholar - optional, increases rate limit
LITREVIEW_PAPER_SOURCES_SEMANTIC_SCHOLAR_API_KEY=

# OpenAlex - optional, provides polite pool access
LITREVIEW_PAPER_SOURCES_OPENALEX_API_KEY=

# Scopus (Elsevier) - required if enabled
LITREVIEW_PAPER_SOURCES_SCOPUS_API_KEY=

# PubMed (NCBI) - optional, increases rate limit from 3/s to 10/s
LITREVIEW_PAPER_SOURCES_PUBMED_API_KEY=

# bioRxiv - optional
LITREVIEW_PAPER_SOURCES_BIORXIV_API_KEY=

# arXiv - optional
LITREVIEW_PAPER_SOURCES_ARXIV_API_KEY=
